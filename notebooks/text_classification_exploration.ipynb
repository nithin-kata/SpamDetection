{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification System - Exploration Notebook\n",
    "\n",
    "This notebook provides an interactive exploration of the text classification system.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Data Loading and Exploration](#data-loading)\n",
    "2. [Text Preprocessing](#preprocessing)\n",
    "3. [Feature Extraction](#features)\n",
    "4. [Model Training](#training)\n",
    "5. [Model Evaluation](#evaluation)\n",
    "6. [Results Visualization](#visualization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Import our custom modules\n",
    "from src.preprocessing import TextPreprocessor, FeatureExtractor\n",
    "from src.models import TextClassifier, ModelTuner\n",
    "from src.evaluation import ModelEvaluator\n",
    "from src.visualization import TextVisualization\n",
    "from data.dataset_loader import load_dataset, get_available_datasets\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Exploration <a id=\"data-loading\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show available datasets\n",
    "datasets = get_available_datasets()\n",
    "print(\"Available datasets:\")\n",
    "for name, info in datasets.items():\n",
    "    print(f\"- {name}: {info['description']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset (change this to try different datasets)\n",
    "dataset_name = '20newsgroups'  # Options: 'sms_spam', '20newsgroups', 'movie_reviews'\n",
    "\n",
    "texts, labels, label_names = load_dataset(dataset_name)\n",
    "\n",
    "print(f\"Dataset: {dataset_name}\")\n",
    "print(f\"Total documents: {len(texts)}\")\n",
    "print(f\"Number of classes: {len(label_names)}\")\n",
    "print(f\"Classes: {list(label_names.values())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize visualization component\n",
    "visualizer = TextVisualization()\n",
    "\n",
    "# Visualize class distribution\n",
    "class_counts = visualizer.plot_class_distribution(\n",
    "    [label_names[label] for label in labels],\n",
    "    title=f\"{dataset_name.title()} Dataset - Class Distribution\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample documents\n",
    "print(\"Sample documents from each class:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for class_idx, class_name in label_names.items():\n",
    "    # Find first document of this class\n",
    "    sample_idx = labels.index(class_idx)\n",
    "    sample_text = texts[sample_idx][:300] + \"...\" if len(texts[sample_idx]) > 300 else texts[sample_idx]\n",
    "    \n",
    "    print(f\"\\nClass: {class_name}\")\n",
    "    print(\"-\" * 30)\n",
    "    print(sample_text)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text Preprocessing <a id=\"preprocessing\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize preprocessor\n",
    "preprocessor = TextPreprocessor()\n",
    "\n",
    "# Show preprocessing example\n",
    "sample_text = texts[0]\n",
    "processed_text = preprocessor.preprocess_text(sample_text)\n",
    "\n",
    "print(\"Preprocessing Example:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Original:\")\n",
    "print(sample_text[:500] + \"...\" if len(sample_text) > 500 else sample_text)\n",
    "print(\"\\nProcessed:\")\n",
    "print(processed_text[:500] + \"...\" if len(processed_text) > 500 else processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess all texts\n",
    "print(\"Preprocessing all texts...\")\n",
    "processed_texts = [preprocessor.preprocess_text(text) for text in texts]\n",
    "print(\"Preprocessing completed!\")\n",
    "\n",
    "# Analyze text length distribution\n",
    "visualizer.plot_text_length_distribution(\n",
    "    processed_texts, \n",
    "    [label_names[label] for label in labels]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Extraction <a id=\"features\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train_text, X_test_text, y_train, y_test = train_test_split(\n",
    "    processed_texts, labels, test_size=0.2, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {len(X_train_text)}\")\n",
    "print(f\"Test set size: {len(X_test_text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize feature extractor\n",
    "feature_extractor = FeatureExtractor()\n",
    "\n",
    "# Extract TF-IDF features\n",
    "print(\"Extracting TF-IDF features...\")\n",
    "X_train_tfidf = feature_extractor.extract_tfidf_features(X_train_text, max_features=5000)\n",
    "X_test_tfidf = feature_extractor.transform_tfidf(X_test_text)\n",
    "\n",
    "print(f\"TF-IDF feature matrix shape: {X_train_tfidf.shape}\")\n",
    "print(f\"Feature density: {X_train_tfidf.nnz / (X_train_tfidf.shape[0] * X_train_tfidf.shape[1]):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Bag of Words features\n",
    "print(\"Extracting Bag of Words features...\")\n",
    "X_train_bow = feature_extractor.extract_bow_features(X_train_text, max_features=5000)\n",
    "X_test_bow = feature_extractor.transform_bow(X_test_text)\n",
    "\n",
    "print(f\"BoW feature matrix shape: {X_train_bow.shape}\")\n",
    "print(f\"Feature density: {X_train_bow.nnz / (X_train_bow.shape[0] * X_train_bow.shape[1]):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Training <a id=\"training\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize evaluator\n",
    "evaluator = ModelEvaluator()\n",
    "\n",
    "# Define models and feature sets to test\n",
    "models_to_test = ['naive_bayes', 'logistic_regression']\n",
    "feature_sets = [\n",
    "    ('TF-IDF', X_train_tfidf, X_test_tfidf),\n",
    "    ('BoW', X_train_bow, X_test_bow)\n",
    "]\n",
    "\n",
    "print(\"Training and evaluating models...\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate all model combinations\n",
    "results = {}\n",
    "\n",
    "for feature_name, X_train_feat, X_test_feat in feature_sets:\n",
    "    print(f\"\\n--- Testing with {feature_name} features ---\")\n",
    "    \n",
    "    for model_type in models_to_test:\n",
    "        model_name = f\"{model_type}_{feature_name}\"\n",
    "        print(f\"\\nTraining {model_name}...\")\n",
    "        \n",
    "        # Train model\n",
    "        classifier = TextClassifier(model_type)\n",
    "        classifier.train(X_train_feat, y_train)\n",
    "        \n",
    "        # Evaluate model\n",
    "        metrics = evaluator.evaluate_model(\n",
    "            classifier, X_test_feat, y_test, model_name\n",
    "        )\n",
    "        \n",
    "        results[model_name] = {\n",
    "            'classifier': classifier,\n",
    "            'metrics': metrics\n",
    "        }\n",
    "        \n",
    "        print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "        print(f\"F1-Score: {metrics['f1_score']:.4f}\")\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation <a id=\"evaluation\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all models\n",
    "comparison_df = evaluator.compare_models()\n",
    "print(\"Model Comparison:\")\n",
    "print(\"=\" * 50)\n",
    "print(comparison_df.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot model comparison\n",
    "evaluator.plot_model_comparison()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best model\n",
    "best_model_name, best_score = evaluator.get_best_model('f1_score')\n",
    "print(f\"Best Model: {best_model_name}\")\n",
    "print(f\"Best F1-Score: {best_score:.4f}\")\n",
    "\n",
    "# Detailed evaluation report\n",
    "evaluator.print_evaluation_report(best_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix for best model\n",
    "evaluator.plot_confusion_matrix(\n",
    "    best_model_name, \n",
    "    class_names=list(label_names.values())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curve if binary classification\n",
    "if len(label_names) == 2:\n",
    "    evaluator.plot_roc_curve(best_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Results Visualization <a id=\"visualization\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance analysis\n",
    "best_classifier = results[best_model_name]['classifier']\n",
    "\n",
    "# Get feature names\n",
    "if 'tfidf' in best_model_name.lower():\n",
    "    feature_names = feature_extractor.tfidf_vectorizer.get_feature_names_out()\n",
    "else:\n",
    "    feature_names = feature_extractor.count_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Get feature importance\n",
    "feature_importance = best_classifier.get_feature_importance(feature_names)\n",
    "if feature_importance:\n",
    "    visualizer.plot_feature_importance(feature_importance, top_n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create word clouds for each class\n",
    "print(\"Generating word clouds...\")\n",
    "for label_idx, class_name in label_names.items():\n",
    "    visualizer.create_wordcloud(\n",
    "        processed_texts, \n",
    "        labels, \n",
    "        class_name=label_idx,\n",
    "        figsize=(12, 6)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction confidence analysis\n",
    "best_result = evaluator.results[best_model_name]\n",
    "visualizer.plot_prediction_confidence(\n",
    "    best_result['y_pred_proba'], \n",
    "    best_result['y_true']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Conclusions\n",
    "\n",
    "This notebook demonstrated a complete text classification pipeline including:\n",
    "\n",
    "1. **Data Loading**: Multiple dataset options with easy switching\n",
    "2. **Preprocessing**: Comprehensive text cleaning and normalization\n",
    "3. **Feature Extraction**: TF-IDF and Bag of Words vectorization\n",
    "4. **Model Training**: Multiple algorithms with systematic evaluation\n",
    "5. **Evaluation**: Comprehensive metrics and visualizations\n",
    "6. **Analysis**: Feature importance and prediction confidence\n",
    "\n",
    "### Key Findings:\n",
    "- Best performing model and its characteristics\n",
    "- Feature extraction method comparison\n",
    "- Most important features for classification\n",
    "- Model strengths and weaknesses\n",
    "\n",
    "### Next Steps:\n",
    "- Try additional models (SVM, Random Forest)\n",
    "- Experiment with different preprocessing techniques\n",
    "- Use word embeddings (Word2Vec, GloVe)\n",
    "- Implement cross-validation\n",
    "- Deploy the best model in production"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}